{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d87510-2e06-4a18-92fa-b8832b715805",
   "metadata": {},
   "source": [
    "#### \n",
    "In the below project the cleaning of the tweets from text column , tokenization of tweets and removal of stop words has been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227b7411-e2b6-4fd5-bd37-891849e3f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15aec84-1718-4412-9ae0-f2eace35a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"D:/Brainworks/Projects/Tweet_analysis/Datasets/train.csv\")\n",
    "df_test=pd.read_csv(\"D:/Brainworks/Projects/Tweet_analysis/Datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746cb0f3-d07f-41b4-9bf9-3eb197c73f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "(3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bd815-98f7-4a01-9e3a-1de53746da55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16fe998-2ff9-4d87-b4bb-938cde9f89a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>7383</td>\n",
       "      <td>obliterate</td>\n",
       "      <td>cedar rapids ia</td>\n",
       "      <td>World of warships makes me mad sometimes but i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2828</td>\n",
       "      <td>cyclone</td>\n",
       "      <td>Geneva</td>\n",
       "      <td>Back to the future in #Vanuatu how Cyclone Pam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>565</td>\n",
       "      <td>arson</td>\n",
       "      <td>Republic of Texas</td>\n",
       "      <td>Arson suspect linked to 30 fires caught in Nor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>594</td>\n",
       "      <td>arson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arson suspect linked to 30 fires caught in Nor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4469</th>\n",
       "      <td>6355</td>\n",
       "      <td>hostages</td>\n",
       "      <td>OK</td>\n",
       "      <td>@TexansDC @kylekrenek @Zepp1978 @Frobeus_NS Ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6285</th>\n",
       "      <td>8978</td>\n",
       "      <td>storm</td>\n",
       "      <td>NC || OR</td>\n",
       "      <td>ice cream + cupcake wars + storm = content sara</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6003</th>\n",
       "      <td>8575</td>\n",
       "      <td>screams</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So @LawsonOfficial just followed me and I cann...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     keyword           location  \\\n",
       "5176  7383  obliterate    cedar rapids ia   \n",
       "1964  2828     cyclone             Geneva   \n",
       "391    565       arson  Republic of Texas   \n",
       "411    594       arson                NaN   \n",
       "4469  6355    hostages                 OK   \n",
       "6285  8978       storm           NC || OR   \n",
       "6003  8575     screams                NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "5176  World of warships makes me mad sometimes but i...       0  \n",
       "1964  Back to the future in #Vanuatu how Cyclone Pam...       1  \n",
       "391   Arson suspect linked to 30 fires caught in Nor...       1  \n",
       "411   Arson suspect linked to 30 fires caught in Nor...       1  \n",
       "4469  @TexansDC @kylekrenek @Zepp1978 @Frobeus_NS Ne...       0  \n",
       "6285    ice cream + cupcake wars + storm = content sara       0  \n",
       "6003  So @LawsonOfficial just followed me and I cann...       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>189</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>In your hearts and minds</td>\n",
       "      <td>@JadeForMKX You should be happy I don't use Af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>7838</td>\n",
       "      <td>quarantine</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Reddit Is Planning to 'Quarantine' Its Most To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>5303</td>\n",
       "      <td>fear</td>\n",
       "      <td>Free Hanseatic City of Bremen, Germany</td>\n",
       "      <td>My grandma had always fear and in some way she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>8485</td>\n",
       "      <td>screamed</td>\n",
       "      <td>trust none..</td>\n",
       "      <td>@CokeBoys__ yo best I screamed when I watched ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>2574</td>\n",
       "      <td>crash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New post: 'India Train Crash Kills Dozens' htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>8958</td>\n",
       "      <td>storm</td>\n",
       "      <td>Florence, SC</td>\n",
       "      <td>The Florence Regional Airport Director says no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>9370</td>\n",
       "      <td>survived</td>\n",
       "      <td>uncanny valley</td>\n",
       "      <td>You'll be pleased to know I survived my dental...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     keyword                                location  \\\n",
       "58     189  aftershock                In your hearts and minds   \n",
       "2345  7838  quarantine                                Maryland   \n",
       "1571  5303        fear  Free Hanseatic City of Bremen, Germany   \n",
       "2544  8485    screamed                            trust none..   \n",
       "779   2574       crash                                     NaN   \n",
       "2688  8958       storm                            Florence, SC   \n",
       "2822  9370    survived                          uncanny valley   \n",
       "\n",
       "                                                   text  \n",
       "58    @JadeForMKX You should be happy I don't use Af...  \n",
       "2345  Reddit Is Planning to 'Quarantine' Its Most To...  \n",
       "1571  My grandma had always fear and in some way she...  \n",
       "2544  @CokeBoys__ yo best I screamed when I watched ...  \n",
       "779   New post: 'India Train Crash Kills Dozens' htt...  \n",
       "2688  The Florence Regional Airport Director says no...  \n",
       "2822  You'll be pleased to know I survived my dental...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.sample(7))\n",
    "print()\n",
    "display(df_test.sample(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2c5914-335b-4481-bbe2-dd36b987a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Train Data:\n",
      "                                                text  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...   \n",
      "1             Forest fire near La Ronge Sask. Canada   \n",
      "2  All residents asked to 'shelter in place' are ...   \n",
      "3  13,000 people receive #wildfires evacuation or...   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "                                          text_clean  \n",
      "0  our deeds are the reason of this earthquake ma...  \n",
      "1              forest fire near la ronge sask canada  \n",
      "2  all residents asked to shelter in place are be...  \n",
      "3  13000 people receive wildfires evacuation orde...  \n",
      "4  just got sent this photo from ruby alaska as s...  \n",
      "------------------------------------------------------------------------------------------\n",
      "Cleaned Test Data:\n",
      "                                                text  \\\n",
      "0                 Just happened a terrible car crash   \n",
      "1  Heard about #earthquake is different cities, s...   \n",
      "2  there is a forest fire at spot pond, geese are...   \n",
      "3           Apocalypse lighting. #Spokane #wildfires   \n",
      "4      Typhoon Soudelor kills 28 in China and Taiwan   \n",
      "\n",
      "                                          text_clean  \n",
      "0                 just happened a terrible car crash  \n",
      "1  heard about earthquake is different cities sta...  \n",
      "2  there is a forest fire at spot pond geese are ...  \n",
      "3              apocalypse lighting spokane wildfires  \n",
      "4      typhoon soudelor kills 28 in china and taiwan  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(\n",
    "        r'['\n",
    "        u'\\U0001F600-\\U0001F64F'\n",
    "        u'\\U0001F300-\\U0001F5FF'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\U0001F1E0-\\U0001F1FF'\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+', \n",
    "        '', \n",
    "        text\n",
    "    )\n",
    "    text = re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "df_train['text_clean'] = df_train['text'].apply(clean_text)\n",
    "df_test['text_clean'] = df_test['text'].apply(clean_text)\n",
    "\n",
    "print(\"Cleaned Train Data:\")\n",
    "print(df_train[['text', 'text_clean']].head())\n",
    "\n",
    "print(\"---\" * 30)\n",
    "\n",
    "print(\"Cleaned Test Data:\")\n",
    "print(df_test[['text', 'text_clean']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef54b92-222b-4c46-9353-5f935707295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "keyword         61\n",
       "location      2533\n",
       "text             0\n",
       "target           0\n",
       "text_clean       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b0526e-8f3c-4e9e-87c3-e7a6139a8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in d:\\brainworks\\projects\\tweet_analysis\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/41.5 kB 682.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.5/41.5 kB 666.1 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.7/57.7 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in d:\\brainworks\\projects\\tweet_analysis\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 0.9/1.5 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 273.6/273.6 kB 16.5 MB/s eta 0:00:00\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 98.2/98.2 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b5b88f0-9ad6-466a-8d61-28df6d9d2ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Anand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b1d7d59-a788-4b73-a1bb-0691bd94dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##we will tokenize the 'text_clean' column\n",
    "\n",
    "df_train['tokenized']=df_train['text_clean'].apply(word_tokenize)\n",
    "df_test['tokenized']=df_test['text_clean'].apply(word_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "617fe237-13d9-4966-8d79-4b6944b4ced7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Tokenized:\n",
      "                                          text_clean  \\\n",
      "0  our deeds are the reason of this earthquake ma...   \n",
      "1              forest fire near la ronge sask canada   \n",
      "2  all residents asked to shelter in place are be...   \n",
      "3  13000 people receive wildfires evacuation orde...   \n",
      "4  just got sent this photo from ruby alaska as s...   \n",
      "\n",
      "                                           tokenized  \n",
      "0  [our, deeds, are, the, reason, of, this, earth...  \n",
      "1      [forest, fire, near, la, ronge, sask, canada]  \n",
      "2  [all, residents, asked, to, shelter, in, place...  \n",
      "3  [13000, people, receive, wildfires, evacuation...  \n",
      "4  [just, got, sent, this, photo, from, ruby, ala...  \n",
      "------------------------------------------------------------------------------------------\n",
      "Test Data Tokenized:\n",
      "                                          text_clean  \\\n",
      "0                 just happened a terrible car crash   \n",
      "1  heard about earthquake is different cities sta...   \n",
      "2  there is a forest fire at spot pond geese are ...   \n",
      "3              apocalypse lighting spokane wildfires   \n",
      "4      typhoon soudelor kills 28 in china and taiwan   \n",
      "\n",
      "                                           tokenized  \n",
      "0          [just, happened, a, terrible, car, crash]  \n",
      "1  [heard, about, earthquake, is, different, citi...  \n",
      "2  [there, is, a, forest, fire, at, spot, pond, g...  \n",
      "3         [apocalypse, lighting, spokane, wildfires]  \n",
      "4  [typhoon, soudelor, kills, 28, in, china, and,...  \n"
     ]
    }
   ],
   "source": [
    "# Display tokenized results\n",
    "print(\"Train Data Tokenized:\")\n",
    "print(df_train[['text_clean', 'tokenized']].head())\n",
    "\n",
    "print(\"---\" * 30)\n",
    "\n",
    "print(\"Test Data Tokenized:\")\n",
    "print(df_test[['text_clean', 'tokenized']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "480e5f45-a1d9-4e66-9064-380c4028dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Brainworks\\Projects\\Tweet_analysis\\venv\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.find('tokenizers/punkt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9d948a0-b026-4515-a164-1e345e73a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:/Brainworks/Projects/Tweet_analysis/venv...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt', download_dir=\"D:/Brainworks/Projects/Tweet_analysis/venv\")\n",
    "import nltk\n",
    "nltk.data.path.append(\"D:/Brainworks/Projects/Tweet_analysis/venv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "799c61b4-8104-4e2a-a210-e50105efc9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>3047</td>\n",
       "      <td>death</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>The first trial in the death of #CecilTheLion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the first trial in the death of cecilthelion w...</td>\n",
       "      <td>[the, first, trial, in, the, death, of, cecilt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6264</th>\n",
       "      <td>8949</td>\n",
       "      <td>storm</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>How to prepare your #property for a #storm:\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>how to prepare your property for a storm</td>\n",
       "      <td>[how, to, prepare, your, property, for, a, storm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>2579</td>\n",
       "      <td>crash</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "      <td>@DestinyTheGame @Bungie @PlayStation Getting k...</td>\n",
       "      <td>0</td>\n",
       "      <td>destinythegame bungie playstation getting kick...</td>\n",
       "      <td>[destinythegame, bungie, playstation, getting,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>3589</td>\n",
       "      <td>desolate</td>\n",
       "      <td>Macclesfield</td>\n",
       "      <td>@booksbyRoger TY for the follow Go To http://t...</td>\n",
       "      <td>0</td>\n",
       "      <td>booksbyroger ty for the follow go to brutally ...</td>\n",
       "      <td>[booksbyroger, ty, for, the, follow, go, to, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6531</th>\n",
       "      <td>9342</td>\n",
       "      <td>survived</td>\n",
       "      <td>London</td>\n",
       "      <td>Survived another tube strike with the last per...</td>\n",
       "      <td>0</td>\n",
       "      <td>survived another tube strike with the last per...</td>\n",
       "      <td>[survived, another, tube, strike, with, the, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   keyword                location  \\\n",
       "2120  3047     death      New York, New York   \n",
       "6264  8949     storm             Chicago, IL   \n",
       "1797  2579     crash    Melbourne, Australia   \n",
       "2499  3589  desolate            Macclesfield   \n",
       "6531  9342  survived                  London   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2120  The first trial in the death of #CecilTheLion ...       0   \n",
       "6264  How to prepare your #property for a #storm:\\n\\...       1   \n",
       "1797  @DestinyTheGame @Bungie @PlayStation Getting k...       0   \n",
       "2499  @booksbyRoger TY for the follow Go To http://t...       0   \n",
       "6531  Survived another tube strike with the last per...       0   \n",
       "\n",
       "                                             text_clean  \\\n",
       "2120  the first trial in the death of cecilthelion w...   \n",
       "6264           how to prepare your property for a storm   \n",
       "1797  destinythegame bungie playstation getting kick...   \n",
       "2499  booksbyroger ty for the follow go to brutally ...   \n",
       "6531  survived another tube strike with the last per...   \n",
       "\n",
       "                                              tokenized  \n",
       "2120  [the, first, trial, in, the, death, of, cecilt...  \n",
       "6264  [how, to, prepare, your, property, for, a, storm]  \n",
       "1797  [destinythegame, bungie, playstation, getting,...  \n",
       "2499  [booksbyroger, ty, for, the, follow, go, to, b...  \n",
       "6531  [survived, another, tube, strike, with, the, l...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>6816</td>\n",
       "      <td>loud%20bang</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>k_matako_bot: Breaking news! Unconfirmed! I ju...</td>\n",
       "      <td>kmatakobot breaking news unconfirmed i just he...</td>\n",
       "      <td>[kmatakobot, breaking, news, unconfirmed, i, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1871</td>\n",
       "      <td>burning</td>\n",
       "      <td>mumbai</td>\n",
       "      <td>hermancranston: WIRED : All these fires are bu...</td>\n",
       "      <td>hermancranston wired all these fires are burni...</td>\n",
       "      <td>[hermancranston, wired, all, these, fires, are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>1358</td>\n",
       "      <td>blown%20up</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@DodgersNation he was due to get blown up at l...</td>\n",
       "      <td>dodgersnation he was due to get blown up at le...</td>\n",
       "      <td>[dodgersnation, he, was, due, to, get, blown, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>6599</td>\n",
       "      <td>inundated</td>\n",
       "      <td>GroÌÙdeutsches Reich</td>\n",
       "      <td>Our children are being inundated with the fals...</td>\n",
       "      <td>our children are being inundated with the fals...</td>\n",
       "      <td>[our, children, are, being, inundated, with, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>922</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>to fight bioterrorism sir</td>\n",
       "      <td>[to, fight, bioterrorism, sir]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword              location  \\\n",
       "2024  6816   loud%20bang                 Kenya   \n",
       "575   1871       burning                mumbai   \n",
       "419   1358    blown%20up                   NaN   \n",
       "1957  6599     inundated  GroÌÙdeutsches Reich   \n",
       "285    922  bioterrorism                   NaN   \n",
       "\n",
       "                                                   text  \\\n",
       "2024  k_matako_bot: Breaking news! Unconfirmed! I ju...   \n",
       "575   hermancranston: WIRED : All these fires are bu...   \n",
       "419   @DodgersNation he was due to get blown up at l...   \n",
       "1957  Our children are being inundated with the fals...   \n",
       "285                          To fight bioterrorism sir.   \n",
       "\n",
       "                                             text_clean  \\\n",
       "2024  kmatakobot breaking news unconfirmed i just he...   \n",
       "575   hermancranston wired all these fires are burni...   \n",
       "419   dodgersnation he was due to get blown up at le...   \n",
       "1957  our children are being inundated with the fals...   \n",
       "285                           to fight bioterrorism sir   \n",
       "\n",
       "                                              tokenized  \n",
       "2024  [kmatakobot, breaking, news, unconfirmed, i, j...  \n",
       "575   [hermancranston, wired, all, these, fires, are...  \n",
       "419   [dodgersnation, he, was, due, to, get, blown, ...  \n",
       "1957  [our, children, are, being, inundated, with, t...  \n",
       "285                      [to, fight, bioterrorism, sir]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.sample(5))\n",
    "print()\n",
    "display(df_test.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba7e1022-db1c-4368-9802-68d59e7936e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###we will do lowercase,Stopword removal, POS tagging, and Wordnet COnversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ec2f0c9-6f45-4627-8b97-22a7207abefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           tokenized  \\\n",
      "0  [our, deeds, are, the, reason, of, this, earth...   \n",
      "1      [forest, fire, near, la, ronge, sask, canada]   \n",
      "2  [all, residents, asked, to, shelter, in, place...   \n",
      "3  [13000, people, receive, wildfires, evacuation...   \n",
      "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
      "\n",
      "                                               lower  \n",
      "0  [our, deeds, are, the, reason, of, this, earth...  \n",
      "1      [forest, fire, near, la, ronge, sask, canada]  \n",
      "2  [all, residents, asked, to, shelter, in, place...  \n",
      "3  [13000, people, receive, wildfires, evacuation...  \n",
      "4  [just, got, sent, this, photo, from, ruby, ala...  \n"
     ]
    }
   ],
   "source": [
    "df_train['lower']=df_train['tokenized'].map(lambda tokens:[word.lower() for word in tokens])\n",
    "\n",
    "##display the first few rows of processed data\n",
    "print(df_train[['tokenized','lower']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "713d25ef-bba8-4f8e-b13e-d99802a12168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                               lower  \n",
       "0  [our, deeds, are, the, reason, of, this, earth...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [all, residents, asked, to, shelter, in, place...  \n",
       "3  [13000, people, receive, wildfires, evacuation...  \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5e1371a-8e40-4329-aea0-1d2956ab17ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               lower  \\\n",
      "0  [our, deeds, are, the, reason, of, this, earth...   \n",
      "1      [forest, fire, near, la, ronge, sask, canada]   \n",
      "2  [all, residents, asked, to, shelter, in, place...   \n",
      "3  [13000, people, receive, wildfires, evacuation...   \n",
      "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
      "\n",
      "                                   stopwords_removed  \n",
      "0  [deeds, reason, earthquake, may, allah, forgiv...  \n",
      "1      [forest, fire, near, la, ronge, sask, canada]  \n",
      "2  [residents, asked, shelter, place, notified, o...  \n",
      "3  [13000, people, receive, wildfires, evacuation...  \n",
      "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "##Ensure stopwords are available\n",
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "###Remove stopwords from the 'lower' column\n",
    "df_train['stopwords_removed']=df_train['lower'].map(lambda tokens:[word for word in tokens if word not in stop])\n",
    "\n",
    "##display the first few rows with stopwords removed\n",
    "print(df_train[['lower','stopwords_removed']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
